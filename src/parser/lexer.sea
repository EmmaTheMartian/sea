use std/result
use parser/keywords

gen Maybe{TokenKind}

rec Lexer(start: ^char, cur: ^char, line: int)

fun make_lexer(code: String): Lexer -> ret new Lexer(code.str, code.str, 1)

// Helpers
#(static, inline) fun l_is_alpha(c: char): bool -> ret (c >= `a` and c <= `z`) or (c >= `A` and c <= `Z`) or c == `_` or c == `$`
#(static, inline) fun l_is_digit(c: char): bool -> ret c >= `0` and c <= `9`
#(static, inline) fun l_is_alnum(c: char): bool -> ret is_alpha(c) or is_digit(c)

#(static, inline) fun l_token(L: ^Lexer, kind: TokenKind): Token -> ret new Token(kind, L^.start, L^.cur - L^.start, L^.line)
#(static, inline) fun l_error_token(L: ^Lexer, msg: String): Token -> ret new Token(TOKEN_ERR, msg.str, msg.len, L^.line)

#(static, inline) fun l_is_done(L: ^Lexer): bool -> ret L^.cur^ == `\0`
#(static, inline) fun l_peek(L: ^Lexer): char -> ret L^.cur^
#(static, inline) fun l_peek_next(L: ^Lexer): char { if is_done(L) -> ret `\0` else -> ret L^.cur[1] }
#(static, inline) fun l_peek_n(L: ^Lexer, offset: int): char -> ret L^.cur[offset]
#(static, inline) fun l_skip(L: ^Lexer) -> L^.cur++
#(static, inline) fun l_advance(L: ^Lexer): char { L^.cur++ ret L^.cur[-1] }

#static
fun skip_whitespace(L: ^Lexer) {
	for true {
		switch l_peek(L) {
			fall case ` ` { }
			fall case `\r` { }
			case `\t` -> l_skip(L)
			case `\n` {
				L^.line++
				l_skip(L)
			}
			else -> ret
		}
	}
}

#static
fun lex_number(L: ^Lexer): Token {
	var floating = false

	for l_is_digit(l_peek(L)) -> l_skip(L)

	// Look for a decimal
	if l_peek(L) == `.` and l_is_digit(l_peek_next(L)) {
		floating = true
		l_skip(L) // Consume the `.`
		for l_is_digit(l_peek(L)) -> l_skip(L)
	}

	if floating -> ret l_token(L, TOKEN_FLOAT)
	else -> ret l_token(L, TOKEN_INT)
}

#static
fun lex_string(L: ^Lexer, cstr: bool): Token {
	l_skip(L) // skip the `"` (or the `c`)
	if cstr -> l_skip(L) // skip the `"` if a c-string

	for l_peek(L) != `"` and not l_is_done(L) {
		if l_peek(L) == `\n` -> L^.line++

		l_skip(L)
	}

	if l_is_done(L) -> ret l_error_token(L, "unterminated string")

	var tok = new Token()
	if cstr {
		tok = l_token(L, TOKEN_C_STRING)
		tok.start++ // ignore the `c`
		tok.len-- // ignore the closing quote
	} else {
		tok = l_token(L, TOKEN_STRING)
	}

	tok.start++ // ignore the first `"`
	tok.len-- // ignore the closing quote
	l_skip(L) // skip the closing `"`

	ret tok
}

#static
fun lex_id_or_keyword(L: ^Lexer): Token {
	for l_is_alnum(l_peek(L)) -> l_skip(L)

	ret l_token(L, check_keyword(L^.start, L^.cur - L^.start))
}

#static
fun lex_char(L: ^Lexer): Token {
	var esc = false

	// if the first character is a backslash then we'll need to parse a second character.
	if l_peek(L) == `\\` {
		l_skip(L)
		esc = true
	}

	l_skip(L) // read the character
	l_skip(L) // skip the closing `

	var tok = l_token(L, TOKEN_CHAR)

	// exclude the opening and closing backticks
	tok.start++
	tok.len = tok.len - 2

	ret tok
}

#static
fun lex_raw_block(L: ^Lexer): Token {
	var depth = 0

	l_skip(L) // skip the opening [

	for depth != 0 {
		let ch = l_advance(L)

		switch ch {
			case `[` -> depth++
			case `]` -> depth--
		}
	}

	ret l_token(L, TOKEN_RAW)
}


fun get_next_token(L: ^Lexer): Token {
	l_skip_whitespace(L)
	L^.start = L^.cur

	if l_is_done(L) -> ret l_token(L, TOKEN_EOF)

	let ch = l_advance(L)
	let next = l_peek(L)

	if l_is_digit(ch) -> ret lex_number(L)
	else if ch == `c` and l_peek(L) == `"` -> ret lex_string(L, true) // C strings
	else if l_is_alpha(ch) {
		if ch == `r` and next == `a` and l_peek_n(L, 2) == `w` {
			L^.start = L^.start + 2
			ret lex_raw_block(L)
		} else {
			ret lex_id_or_keyword(L)
		}
	}
	else {
		switch ch {
			// Single-character tokens
			case `"` -> ret lex_string(L, false)
			case `\`` -> ret lex_char(L)
			case `,` -> ret l_token(L, TOKEN_COMMA)
			case `:` -> ret l_token(L, TOKEN_COLON)
			case `;` -> ret l_token(L, TOKEN_SEMI)
			case `^` -> ret l_token(L, TOKEN_PTR)
			case `(` -> ret l_token(L, TOKEN_OPEN_PAREN)
			case `)` -> ret l_token(L, TOKEN_CLOSE_PAREN)
			case `[` -> ret l_token(L, TOKEN_OPEN_BRACKET)
			case `]` -> ret l_token(L, TOKEN_CLOSE_BRACKET)
			case `{` -> ret l_token(L, TOKEN_OPEN_BRACE)
			case `}` -> ret l_token(L, TOKEN_CLOSE_BRACE)
			case `#` -> ret l_token(L, TOKEN_HASH)
			// Operators (these get funky since some are two characters)
			case `.` -> ret l_token(L, TOKEN_OP_DOT)
			case `-` {
				// ->
				if next == `>` {
					l_skip(L)
					ret l_token(L, TOKEN_PIPE)
				}
				// --
				else if next == `-` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_DEC)
				}
				// -
				ret l_token(L, TOKEN_OP_SUB)
			}
			case `=` {
				// ==
				if next == `=` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_EQ)
				}
				// =
				ret l_token(L, TOKEN_EQ)
			}
			case `!` {
				// !=
				if next == `=` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_NEQ)
				}
			}
			case `>` {
				// >=
				if next == `=` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_GTEQ)
				}
				// >
				ret l_token(L, TOKEN_OP_GT)
			}
			case `<` {
				// <=
				if next == `=` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_LTEQ)
				}
				// <
				ret l_token(L, TOKEN_OP_LT)
			}
			case `+` {
				// ++
				if next == `+` {
					l_skip(L)
					ret l_token(L, TOKEN_OP_INC)
				}
				// +
				ret l_token(L, TOKEN_OP_ADD)
			}
			case `*` -> ret l_token(L, TOKEN_OP_MUL)
			case `/` -> ret l_token(L, TOKEN_OP_DIV)
			case `%` -> ret l_token(L, TOKEN_OP_MOD)
		}
	}

	ret l_error_token(L, "unexpected character")
}
